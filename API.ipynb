{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyteomics.mzml\n",
    "import spectrum_utils.spectrum as sus\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_msfragger_protein(protein_file_path):\n",
    "    # load the protein file into a pandas dataframe\n",
    "    protein_dataframe = pd.read_table(protein_file_path)\n",
    "\n",
    "    # Rename the \"Protein ID\" column to faciliate merging\n",
    "    protein_dataframe = protein_dataframe.rename({'Protein ID': 'Protein Accession'}, axis=1)\n",
    "    \n",
    "    return protein_dataframe\n",
    "def load_protein(protein_file_path):\n",
    "    # load the protein file into a pandas dataframe\n",
    "    protein_dataframe = pd.read_table(protein_file_path)\n",
    "    return protein_dataframe\n",
    "def join_peptideQ_and_protein_dataframes(protein_df, peptideQ_df):\n",
    "    # join based on the \"Protein Accession\"\n",
    "    joined_dataframe = peptideQ_df.merge(right=protein_df, on=\"Protein Accession\", how='inner', suffixes=('_protein', '_peptide'))\n",
    "    \n",
    "    # generate multiIndex\n",
    "    #joined_dataframe = joined_dataframe.set_index(['Protein Accession', 'Peptide'])\n",
    "    \n",
    "    return joined_dataframe\n",
    "def load_msfragger_peptideQ(peptideQ_file_path):\n",
    "    # read the peptideQ file into a pandas dataframe\n",
    "    peptideQ_dataframe = pd.read_table(peptideQ_file_path, delimiter='\\t')\n",
    "\n",
    "    # rename the \"Protein Groups\" header so we can use the df.merge function later\n",
    "    peptideQ_dataframe = peptideQ_dataframe.rename({'Protein ID': 'Protein Accession', 'Peptide Sequence': 'Peptide'}, axis=1)\n",
    "    \n",
    "    return peptideQ_dataframe\n",
    "def load_peptideQ(peptideQ_file_path):\n",
    "    # read the peptideQ file into a pandas dataframe\n",
    "    peptideQ_dataframe = pd.read_table(peptideQ_file_path, delimiter='\\t')\n",
    "\n",
    "    # rename the \"Protein Groups\" header so we can use the df.merge function later\n",
    "    peptideQ_dataframe = peptideQ_dataframe.rename({\"Protein Groups\": \"Protein Accession\", \"Sequence\" : \"Peptide\"}, axis=1)\n",
    "    \n",
    "    return peptideQ_dataframe\n",
    "def join_psm_and_peptideQ_dataframes(psm_df, peptideQ_df):\n",
    "    # find all the duplicate columns that are not the 'Peptide'\n",
    "    duplicate_columns = []\n",
    "    for column in psm_df.columns:\n",
    "        if column in peptideQ_df.columns and column != 'Peptide':\n",
    "            duplicate_columns.append(column)\n",
    "        \n",
    "    psm_df = psm_df.drop(columns=duplicate_columns)\n",
    "\n",
    "    # join based on the \"Base Sequence\"\n",
    "    joined_dataframe = psm_df.merge(right=peptideQ_df, on=\"Peptide\", how='inner', )\n",
    "\n",
    "    # generate multiIndex\n",
    "    #joined_dataframe = joined_dataframe.set_index(['File Name','Protein Accession','Peptide', 'Scan Number']).drop(columns=[\"Protein Groups\"])\n",
    "\n",
    "    return joined_dataframe\n",
    "def load_psm(psm_file_path):\n",
    "    # read the psm file into a pandas dataframe\n",
    "    psm_dataframe = pd.read_table(psm_file_path, delimiter='\\t')\n",
    "\n",
    "    # sort dataframe by QValue\n",
    "    psm_dataframe = psm_dataframe.sort_values(\"QValue\")\n",
    "\n",
    "    # drop duplicates\n",
    "    psm_dataframe = psm_dataframe.drop_duplicates(subset=[\"Scan Number\"], keep=\"first\")\n",
    "    psm_dataframe[\"Protein Accession\"] = psm_dataframe[\"Protein Accession\"].astype(str)\n",
    "\n",
    "    # rename the \"Full Sequence\" column\n",
    "    psm_dataframe = psm_dataframe.rename({\"Full Sequence\": \"Peptide\"}, axis=1)\n",
    "\n",
    "    return psm_dataframe\n",
    "def load_psm_df_msfragger(psm_file_path):\n",
    "    # read in the psm file as a dataframe\n",
    "    psm_df = pd.read_table(psm_file_path)\n",
    "\n",
    "    # split the \"Spectrum\" column into a list at each period and store it under \n",
    "    # the \"temp_split_column\"\n",
    "    psm_df[\"temp_split_column\"] = psm_df[\"Spectrum\"].str.split(\".\")\n",
    "    # store the element located at index 1 of the \"temp_split_column\" in a \n",
    "    # \"Scan Number\" column\n",
    "    psm_df[\"Scan Number\"] = psm_df[\"temp_split_column\"].map(lambda x:x[1]).apply(pd.to_numeric)\n",
    "    # drop unneeded columns\n",
    "    columns_to_drop = ['Spectrum', 'temp_split_column','Mapped Genes', 'Mapped Proteins']\n",
    "    psm_df = psm_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # rename 2 headers to match MM file formats\n",
    "    psm_df = psm_df.rename({'Protein ID': 'Protein Accession', 'Spectrum File': 'File Name'}, axis=1)\n",
    "\n",
    "    # drop duplicates\n",
    "    psm_df = psm_df.drop_duplicates(subset=[\"Scan Number\"], keep=\"first\")\n",
    "    \n",
    "    return psm_df\n",
    "\n",
    "def load_mzml_df(mzml_file_path):\n",
    "    # use pyteomics.mzml.read() to generate an iterator over the dicts with spectrum properties\n",
    "    mzml_dicts = pyteomics.mzml.read(source=mzml_file_path)\n",
    "\n",
    "    # load dataframe from the list of mzml dictionaires\n",
    "    # drop the extra index column\n",
    "    mzml_df = pd.DataFrame(mzml_dicts).drop(columns='index')\n",
    "\n",
    "    # create a new dataframe containing only the ms/ms scans\n",
    "    relevant_info = mzml_df.loc[(mzml_df['ms level'] == 2)]\n",
    "\n",
    "    # reset the index to make up for the ms scans that were not included in this database\n",
    "    relevant_info = relevant_info.reset_index(drop=True)\n",
    "\n",
    "    # drop irrelevent columns (Note: We can change this if needed.)\n",
    "    columns_to_drop = [\"spectrum title\", \"count\", \"positive scan\", \"centroid spectrum\", \"defaultArrayLength\", \"MSn spectrum\", \"dataProcessingRef\", \"scanList\", \"MS1 spectrum\", \"ms level\"]\n",
    "    relevant_info = relevant_info.drop(columns=columns_to_drop)\n",
    "\n",
    "    # create a new \"Scan Number\" column\n",
    "    # the scan number info is contained within the \"id\" column so we will pull out the scan number and then delete the \"id\" column\n",
    "    relevant_info[\"temp_split_column\"] = (relevant_info[\"id\"].str.split(\" \"))\n",
    "    relevant_info[\"Scan Number\"] = relevant_info[\"temp_split_column\"].map(lambda x:x[2]).str.replace(\"scan=\", \"\")\n",
    "    relevant_info[\"Scan Number\"] = relevant_info[\"Scan Number\"].apply(pd.to_numeric)\n",
    "    relevant_info = relevant_info.drop(columns=['temp_split_column', 'id'])\n",
    "\n",
    "    # next, we'll want to pull out some info about the precursor in the \"precursorList\" columm\n",
    "    # we will store the info we want under the \"precursor info\" column\n",
    "    # then we'll drop the \"precursorList\" column\n",
    "    relevant_info[\"precursor info\"] = relevant_info[\"precursorList\"].map(lambda x:x['precursor'][0]['selectedIonList']['selectedIon'][0]).astype(str)\n",
    "    relevant_info = relevant_info.drop(columns=[\"precursorList\"])\n",
    "\n",
    "    # the precursor info is stored as a string in the format of a dictionary\n",
    "    # json.loads() requires \" instead of ' so we will fix that then convert the string into a dictionary\n",
    "    dict_list = relevant_info[\"precursor info\"].tolist()\n",
    "\n",
    "    for index, dictionary in enumerate(dict_list):\n",
    "        dictionary = dictionary.replace(\"'\", '\"')\n",
    "        dict_list[index] = json.loads(dictionary)\n",
    "\n",
    "    # we'll then load the dictionary data into a temporary dataframe\n",
    "    three_column_df = pd.DataFrame.from_dict(dict_list)\n",
    "\n",
    "    # next, we'll concatenate these two dataframes along the columns based on the index\n",
    "    complete_mzml_df = pd.concat([relevant_info, three_column_df], axis=\"columns\")\n",
    "\n",
    "    # drop the \"precursor info\" column because we don't need it anymore\n",
    "    complete_mzml_df = complete_mzml_df.drop(columns=['precursor info'])\n",
    "\n",
    "    # as the scan number is the index we care about, we will use it as our index in the dataframe\n",
    "    complete_mzml_df= complete_mzml_df.set_index(\"Scan Number\")\n",
    "\n",
    "    return complete_mzml_df \n",
    "def save_df(joined_dataframe, file_path):\n",
    "    joined_dataframe.to_csv(file_path, sep=\"\\t\", index=False)\n",
    "    print(f'Dataframe saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psm_and_peptideQ_controller(file_type, psm_file_path, peptideQ_file_path, columns_to_keep=None):\n",
    "    \n",
    "    ''' Joins a psm and a Peptide Quantification file. Files are joined into a pandas dataframe and saved as a tsv.\n",
    "    \n",
    "    Required Parameters:\n",
    "        * file_type: \"mm\" for metamorpheus files, or \"msfragger\" for msfragger files\n",
    "        * psm_file_path: File path to the psm file\n",
    "        * peptideQ_file_path: File path to the Peptide Quantification file\n",
    "        * psm_and_peptideQ_file_path: Output file path\n",
    "        \n",
    "    Optional Parameters: \n",
    "        * columns_to_keep: List of columns to include in the dataframe. Note that column names may vary based on whether your files were generated with MetaMorpheus or MSFragger.'''\n",
    "        \n",
    "    # load dataframes\n",
    "\n",
    "    if file_type.lower() == 'mm':\n",
    "        psm_df = load_psm(psm_file_path)\n",
    "        peptideQ_df = load_peptideQ(peptideQ_file_path)\n",
    "    elif file_type.lower() == 'msfragger':\n",
    "        peptideQ_df = load_msfragger_peptideQ(peptideQ_file_path=peptideQ_file_path)\n",
    "        psm_df = load_psm_df_msfragger(psm_file_path)\n",
    "    else:\n",
    "        print('invalid file type')\n",
    "        return\n",
    "    \n",
    "\n",
    "    # join dataframes\n",
    "    \n",
    "    joined_df = join_psm_and_peptideQ_dataframes(psm_df, peptideQ_df)\n",
    "\n",
    "    # select all columns to keep, if this parameter was not passed in, return dataframe with all columns\n",
    "    if columns_to_keep != None:\n",
    "        joined_df = joined_df[columns_to_keep]\n",
    "    \n",
    "    return joined_df\n",
    "def mzml_and_psm_controller(file_type, mzml_file_path,psm_file_path, columns_to_keep=None):\n",
    "\n",
    "    ''' Joins an mzml and psm file. Files are joined into a pandas dataframe and saved as a tsv.\n",
    "    \n",
    "    Required Parameters:\n",
    "        * file_type: \"mm\" for metamorpheus files, or \"msfragger\" for msfragger files\n",
    "        * mzml_file_path: File path to the mzML file\n",
    "        * psm_file_path: File path to the psm file\n",
    "        * mzml_and_psm_file_path: Output file path\n",
    "        \n",
    "    Optional Parameters: \n",
    "        * columns_to_keep: List of columns to include in the dataframe. Note that column names may vary based on whether your files were generated with MetaMorpheus or MSFragger.'''\n",
    "\n",
    "    # load psm dataframe based on psm file type\n",
    "    if file_type.lower() == 'mm':\n",
    "        psm_dataframe = load_psm(psm_file_path)\n",
    "    elif file_type.lower() == 'msfragger':\n",
    "        psm_dataframe = load_psm_df_msfragger(psm_file_path=psm_file_path)\n",
    "    else:\n",
    "        print('invalid file type')\n",
    "        return \n",
    "    \n",
    "    # load mzML dataframe\n",
    "    mzml_dataframe = load_mzml_df(mzml_file_path)\n",
    "\n",
    "    # merge datafames based on \"Scan Number\"\n",
    "    joined_dataframe = mzml_dataframe.join(other=psm_dataframe, on='Scan Number', how='inner')\n",
    "\n",
    "    # select all columns to keep, if this parameter was not passed in, return dataframe with all columns\n",
    "    if columns_to_keep != None:\n",
    "        joined_dataframe = joined_dataframe[columns_to_keep]\n",
    "  \n",
    "    return joined_dataframe\n",
    "    \n",
    "def peptideQ_and_protein_controller (file_type, peptideQ_file_path, protein_file_path, columns_to_keep=None):\n",
    "    \n",
    "    ''' Joins a Peptide and Protein Quantification files. Files are joined into a pandas dataframe and saved as a tsv.\n",
    "    \n",
    "    Required Parameters:\n",
    "        * file_type: \"mm\" for metamorpheus files, or \"msfragger\" for msfragger files\n",
    "        * peptideQ_file_path: File path to the Peptide Quantification file\n",
    "        * protein_file_path: File path to the Protein Quantification file\n",
    "        * peptideQ_and_protein_file_path: Output file path\n",
    "        \n",
    "    Optional Parameters: \n",
    "        * columns_to_keep: List of columns to include in the dataframe. Note that column names may vary based on whether your files were generated with MetaMorpheus or MSFragger. '''\n",
    "\n",
    "    # load dataframes\n",
    "    if file_type.lower() == 'mm':\n",
    "        protein_df = load_protein(protein_file_path)\n",
    "        peptideQ_df = load_peptideQ(peptideQ_file_path)\n",
    "    elif file_type.lower() == 'msfragger':\n",
    "        peptideQ_df = load_msfragger_peptideQ(peptideQ_file_path=peptideQ_file_path)\n",
    "        protein_df = load_msfragger_protein(protein_file_path=protein_file_path)\n",
    "    else:\n",
    "        print('invalid file type')\n",
    "        return\n",
    "    \n",
    "\n",
    "    # create joined dataframe and save as csv\n",
    "    joined_df = join_peptideQ_and_protein_dataframes(protein_df=protein_df, peptideQ_df=peptideQ_df)\n",
    "\n",
    "    # select all columns to keep, if this parameter was not passed in, return dataframe with all columns\n",
    "    if columns_to_keep != None:\n",
    "        joined_df = joined_df[columns_to_keep]\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mzml_psm_and_peptideQ_controller(file_type, mzml_file_path, psm_file_path, peptideQ_file_path, columns_to_keep=None):\n",
    "    # merge mzml and psm dataframes\n",
    "    mzml_and_psm_df = mzml_and_psm_controller(file_type=file_type, mzml_file_path=mzml_file_path, psm_file_path=psm_file_path, columns_to_keep=None)\n",
    "    \n",
    "    # load peptideQ dataframe\n",
    "    if file_type.lower() == 'mm':\n",
    "        peptideQ_df = load_peptideQ(peptideQ_file_path)\n",
    "    elif file_type.lower() == 'msfragger':\n",
    "        peptideQ_df = load_msfragger_peptideQ(peptideQ_file_path=peptideQ_file_path)\n",
    "    else:\n",
    "        print('invalid file type')\n",
    "        return\n",
    "    \n",
    "    # merge dataframes\n",
    "    merged_df = join_psm_and_peptideQ_dataframes(psm_df=mzml_and_psm_df, peptideQ_df=peptideQ_df)\n",
    "    # get rid of duplicate entries and columns\n",
    "    if 'Peptide_x' in merged_df.columns and 'Peptide_y' in merged_df.columns and 'Protein Accession_x' in merged_df.columns and 'Protein Accession_y' in merged_df.columns:\n",
    "        merged_df = merged_df.drop_duplicates(subset=[\"Scan Number\"], keep=\"first\")\n",
    "        merged_df = merged_df.rename({'Peptide_x' : 'Peptide', 'Protein Accession_x': 'Protein Accession'}, axis=1)\n",
    "        merged_df = merged_df.drop(columns=['Peptide_y', 'Protein Accession_y'])\n",
    "\n",
    "    print (merged_df.columns)\n",
    "    merged_df = merged_df.set_index(['File Name', 'Protein Accession', 'Peptide', 'Scan Number'])\n",
    "    \n",
    "    # select all columns to keep, if this parameter was not passed in, return dataframe with all columns\n",
    "    if columns_to_keep != None:\n",
    "        merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "    return merged_df\n",
    "def psm_peptideQ_and_protein_controller(file_type, psm_file_path, peptideQ_file_path, protein_file_path,columns_to_keep=None):\n",
    "    # merge psm and peptideQ dataframes\n",
    "    psm_and_peptideQ_df = psm_and_peptideQ_controller(file_type=file_type, psm_file_path=psm_file_path, peptideQ_file_path=peptideQ_file_path)\n",
    "\n",
    "    # load protein dataframe\n",
    "    if file_type.lower() == 'mm':\n",
    "        protein_df = load_protein(protein_file_path)\n",
    "    elif file_type.lower() == 'msfragger':\n",
    "        protein_df = load_msfragger_protein(protein_file_path=protein_file_path)\n",
    "    else:\n",
    "        print('invalid file type')\n",
    "        return\n",
    "    \n",
    "    # merge all dataframes\n",
    "    merged_df = join_peptideQ_and_protein_dataframes(protein_df=protein_df, peptideQ_df=psm_and_peptideQ_df)\n",
    "\n",
    "    # select all columns to keep, if this parameter was not passed in, return dataframe with all columns\n",
    "    if columns_to_keep != None:\n",
    "        merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mzML file\n",
    "mzml_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\MetaM\\\\2ng\\\\Ex_Auto_J3_30umTB_2ngQC_60m_1.mzML\"\n",
    "\n",
    "# MetaMorpheus files\n",
    "mm_psm_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\MetaM\\\\2ng\\\\Ex_Auto_J3_30umTB_2ngQC_60m_1-calib_PSMs.psmtsv\"\n",
    "mm_peptideQ_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\MetaM\\\\2ng\\\\AllQuantifiedPeptides.tsv\"\n",
    "mm_protein_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\MetaM\\\\2ng\\\\AllQuantifiedProteinGroups.tsv\"\n",
    "\n",
    "# MSFragger files\n",
    "msfragger_psm_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\msfragger\\\\psm1.tsv\"\n",
    "msfragger_peptide_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\msfragger\\\\combined_peptide.tsv\"\n",
    "msfragger_protein_file_path = \"C:\\\\Users\\\\Sarah Curtis\\\\OneDrive - BYU\\\\Documents\\\\Single Cell Team Documents\\\\API_dev\\\\msfragger\\\\combined_protein.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_df_controller(file_type, mzml_file_path, psm_file_path, peptideQ_file_path, protein_file_path, columns_to_keep=None):\n",
    "    # merge mzml and psm dataframes\n",
    "    mzml_and_psm_df = mzml_and_psm_controller(file_type=file_type, mzml_file_path=mzml_file_path, psm_file_path=psm_file_path)\n",
    "\n",
    "    # merge peptideQ and protein dataframes\n",
    "    peptideQ_and_protein_df = peptideQ_and_protein_controller(file_type=file_type, peptideQ_file_path=peptideQ_file_path, protein_file_path=protein_file_path)\n",
    "\n",
    "    # merge all data\n",
    "    merged_df = join_psm_and_peptideQ_dataframes(psm_df=mzml_and_psm_df, peptideQ_df=peptideQ_and_protein_df)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files(file_type, input_files, output_file_path, columns_to_keep=None, multiIndex=None, proteins_to_keep=None, peptides_to_keep=None):\n",
    "    # check file_type variable\n",
    "    valid_file_type = check_file_type(file_type)\n",
    "    if not valid_file_type:\n",
    "        return False\n",
    "    # check rows_to_keep parameter\n",
    "    \n",
    "    # interpret the file list\n",
    "    interpreted_file_list = assign_file_types(input_files)\n",
    "\n",
    "    # check that the input_files contained a valid list of files\n",
    "    bool_file_list = generate_bool_file_list(interpreted_file_list)\n",
    "    call_dictionary = load_call_dictionary(file_type, interpreted_file_list, columns_to_keep)\n",
    "    if bool_file_list not in call_dictionary.keys():\n",
    "        print(\"Invalid file combination\")\n",
    "        return False\n",
    "    \n",
    "    # using the call dictionary, call the correct controller function with the associated parameters\n",
    "    function, parameters, default_multiIndex = call_dictionary[bool_file_list]\n",
    "    if type(parameters) == list:\n",
    "        user_dataframe = function(*parameters)\n",
    "    else:\n",
    "        user_dataframe = function(parameters)\n",
    "\n",
    "    # columns to keep\n",
    "    user_dataframe = select_columns_to_keep(user_dataframe=user_dataframe, columns_to_keep=columns_to_keep)\n",
    "\n",
    "    # proteins to keep\n",
    "    if proteins_to_keep != None:\n",
    "        user_dataframe = user_dataframe.loc[user_dataframe['Protein Accession'].isin(proteins_to_keep)]\n",
    "\n",
    "    # peptides to keep\n",
    "    if peptides_to_keep != None:\n",
    "        user_dataframe = user_dataframe.loc[user_dataframe['Peptide'].isin(peptides_to_keep)]\n",
    "    \n",
    "    # multiIndexing\n",
    "    user_dataframe = select_multiIndex(user_dataframe=user_dataframe, multiIndex=multiIndex, default_multiIndex=default_multiIndex)\n",
    "    \n",
    "    # things that still need to be done...\n",
    "    #   * add comments to the new code as well as helper stuff for this function and the other controller ones\n",
    "    #   * fix the mzml stuff\n",
    "    #   * add in the helper part of the function a list of headers\n",
    "    save_df(joined_dataframe=user_dataframe, file_path=output_file_path)\n",
    "\n",
    "    return user_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_file_types(input_files):\n",
    "\n",
    "    file_path_list = [None, None, None, None]\n",
    "\n",
    "    for index, file_path in enumerate(input_files):\n",
    "        file_name = os.path.basename(file_path).lower()\n",
    "        if 'protein' in file_name or 'prot' in file_name:\n",
    "            file_path_list[3] = file_path\n",
    "        elif 'peptide' in file_name or 'pep' in file_name:\n",
    "            file_path_list[2] = file_path\n",
    "        elif 'psm' in file_name:\n",
    "            file_path_list[1] = file_path\n",
    "        elif 'mzml' in file_path:\n",
    "            file_path_list[0] = file_path\n",
    "        else:\n",
    "            # most of the mzML files do not contain the word 'mzml'\n",
    "            file_path_list[0] = file_path\n",
    "    \n",
    "    return file_path_list\n",
    "\n",
    "def generate_bool_file_list(interpreted_file_list):\n",
    "    bool_file_list = []\n",
    "    for file_path in interpreted_file_list:\n",
    "        if file_path:\n",
    "            bool_file_list.append(1)\n",
    "        else:\n",
    "            bool_file_list.append(0)\n",
    "\n",
    "    bool_file_list = str(bool_file_list).replace(\" \", \"\")\n",
    "    return bool_file_list\n",
    "\n",
    "def load_call_dictionary(file_type, interpreted_file_list, columns_to_keep):\n",
    "    call_dictionary = {}\n",
    "\n",
    "    # load one dataframe\n",
    "    # fix how the load mzml thing works\n",
    "    call_dictionary['[1,0,0,0]'] =  [load_mzml_df, interpreted_file_list[0], ['Scan Number']]\n",
    "    if file_type == \"mm\":\n",
    "        call_dictionary['[0,1,0,0]'] = [load_psm, interpreted_file_list[1], ['Protein Accession','Peptide', 'Scan Number']]\n",
    "        call_dictionary['[0,0,1,0]'] = [load_peptideQ, interpreted_file_list[2], ['Protein Accession', 'Peptide']]\n",
    "        call_dictionary['[0,0,0,1]'] = [load_protein, interpreted_file_list[3], ['Protein Accession']]\n",
    "    else:\n",
    "        call_dictionary['[0,1,0,0]'] = [load_psm_df_msfragger, interpreted_file_list[1], ['Protein Accession','Peptide', 'Scan Number']]\n",
    "        call_dictionary['[0,0,1,0]'] = [load_msfragger_peptideQ, interpreted_file_list[2], ['Protein Accession', 'Peptide']]\n",
    "        call_dictionary['[0,0,0,1]'] = [load_msfragger_protein, interpreted_file_list[3], ['Protein Accession']]\n",
    "    \n",
    "    # load and merge 2 dataframes\n",
    "    call_dictionary['[1,1,0,0]'] = [mzml_and_psm_controller, [file_type, interpreted_file_list[0], interpreted_file_list[1], columns_to_keep],\n",
    "    ['Protein Accession','Peptide', 'Scan Number']]\n",
    "    call_dictionary['[0,1,1,0]'] = [psm_and_peptideQ_controller, [file_type, interpreted_file_list[1], interpreted_file_list[2], columns_to_keep],\n",
    "    ['Protein Accession','Peptide', 'Scan Number']]\n",
    "    call_dictionary['[0,0,1,1]'] = [peptideQ_and_protein_controller, [file_type, interpreted_file_list[2], interpreted_file_list[3], columns_to_keep],\n",
    "    ['Protein Accession', 'Peptide']]\n",
    "\n",
    "    # load and merge 3 dataframes\n",
    "    call_dictionary['[1,1,1,0]'] = [mzml_psm_and_peptideQ_controller, [file_type, interpreted_file_list[0],interpreted_file_list[1], interpreted_file_list[2], columns_to_keep],\n",
    "    ['Protein Accession','Peptide', 'Scan Number']]\n",
    "    call_dictionary['[0,1,1,1]'] = [psm_peptideQ_and_protein_controller, [file_type, interpreted_file_list[1],interpreted_file_list[2], interpreted_file_list[3], columns_to_keep],\n",
    "    ['Protein Accession','Peptide', 'Scan Number']]\n",
    "\n",
    "    # load and merge all 3 dataframes\n",
    "    call_dictionary['1,1,1,1'] = [master_df_controller, [file_type, interpreted_file_list[0],interpreted_file_list[1], interpreted_file_list[2], interpreted_file_list[3],columns_to_keep],\n",
    "    ['Protein Accession','Peptide', 'Scan Number']]\n",
    "\n",
    "    return call_dictionary\n",
    "\n",
    "def check_file_type(file_type):\n",
    "    acceptable_types = ['mm', 'msfragger', 'NA']\n",
    "    if file_type not in acceptable_types:\n",
    "        print(f\"Invalid file_type. Acceptable file types are {acceptable_types}\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def select_columns_to_keep(user_dataframe, columns_to_keep):\n",
    "    if columns_to_keep != None:\n",
    "        # check that all of the columns listed exist in the user database\n",
    "        mistake = False\n",
    "        for column in columns_to_keep:\n",
    "            if column not in user_dataframe.columns:\n",
    "                mistake = True\n",
    "                print(f\"{column} column does not exist in this dataframe.\")\n",
    "        if mistake:\n",
    "            print(\"To ensure that you are given all the infomation needed, the entire database will be returned.\")\n",
    "        else:\n",
    "            user_dataframe = user_dataframe[columns_to_keep]\n",
    "    return user_dataframe\n",
    "\n",
    "def select_multiIndex(user_dataframe, multiIndex, default_multiIndex):\n",
    "    if multiIndex != None:\n",
    "        # check that the columns exist\n",
    "        adjusted_default_multiIndex = multiIndex.copy()\n",
    "        for column in multiIndex:\n",
    "            if column not in user_dataframe.columns:\n",
    "                adjusted_default_multiIndex.remove(column)\n",
    "                print(f\"{column} column does not exist in this dataframe and cannot be used as an index\")\n",
    "        user_dataframe = user_dataframe.set_index(adjusted_default_multiIndex)\n",
    "    else:\n",
    "        adjusted_default_multiIndex = default_multiIndex.copy()\n",
    "        for column in default_multiIndex:\n",
    "            if column not in user_dataframe.columns:\n",
    "                adjusted_default_multiIndex.remove(column)\n",
    "        user_dataframe = user_dataframe.set_index(adjusted_default_multiIndex)\n",
    "    return user_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QValue column does not exist in this dataframe and cannot be used as an index\n",
      "Dataframe saved.\n"
     ]
    }
   ],
   "source": [
    "myList = [msfragger_psm_file_path]\n",
    "test10 = parse_files(file_type='msfragger', input_files=myList, output_file_path='asdf', columns_to_keep=[\"Scan Number\", \"Charge\"], multiIndex=[\"Scan Number\", \"QValue\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scan Number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4722</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20457</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20487</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20534</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20783</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6925 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Charge\n",
       "Scan Number        \n",
       "4197              2\n",
       "4699              3\n",
       "4722              2\n",
       "4731              3\n",
       "4742              3\n",
       "...             ...\n",
       "20457             2\n",
       "20487             2\n",
       "20534             2\n",
       "20541             2\n",
       "20783             2\n",
       "\n",
       "[6925 rows x 1 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid file combination.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_file_types(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on the parser, pretty much done and just working on testing it to check for bugs\n",
    "\n",
    "# Things to add to the function\n",
    "#   * split the function based on ms and mm\n",
    "#   * add in a parameter that allows you to specify a list of peptides, proteins\n",
    "\n",
    "\n",
    "# Identifying key features of single-cell spectra to improve "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb791f40de0ed417c7668bedc816e2881564503a427da7fde26581559a099aa2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
